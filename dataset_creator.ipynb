{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mtcnn import MTCNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12890bb2098783f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Face Detection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1cf7610ae031ab6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Base directory where the emotion folders are located\n",
    "base_dir = \"Datasets/RAF-FER-SFEW-AN\"\n",
    "# New dataset directory\n",
    "new_dataset_dir = \"Datasets/combined_dataset_faces_only_mtcnn\"\n",
    "\n",
    "# Emotion classes\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Create directories for the new dataset\n",
    "if not os.path.exists(new_dataset_dir):\n",
    "    os.makedirs(new_dataset_dir)\n",
    "for emotion in emotions:\n",
    "    os.makedirs(os.path.join(new_dataset_dir, emotion), exist_ok=True)\n",
    "    \n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(base_dir, emotion)\n",
    "    all_images = os.listdir(emotion_dir)\n",
    "    existing_dir = os.path.join(new_dataset_dir, emotion)\n",
    "    existing_images = os.listdir(existing_dir)\n",
    "    all_images = [img for img in all_images if img not in existing_images]\n",
    "    \n",
    "    for image in all_images:\n",
    "        img_path = os.path.join(emotion_dir, image)\n",
    "        try:\n",
    "            img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "            detector = MTCNN()\n",
    "            \n",
    "            # Increment correct count if prediction matches the folder name\n",
    "            if detector.detect_faces(img):\n",
    "                # Copy the correctly identified image to the new dataset\n",
    "                shutil.copy(img_path, os.path.join(new_dataset_dir, emotion, image))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aaa14ede2389ea4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base Model - Data Preparation\n",
    "Face Detection\n",
    "Image Cropping, Alignment and Grayscaling\n",
    "Rescaling and Padding\n",
    "128, 128, 1 grayscale images\n",
    "\n",
    "This code only needs to be run once to create the dataset used for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302d48e7e18c6c2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize MTCNN detector\n",
    "detector = MTCNN()\n",
    "\n",
    "base_dir = \"Datasets/combined_dataset_faces_only_mtcnn\"\n",
    "processed_folder = \"Datasets/combined_dataset_processed_128_1\"\n",
    "\n",
    "# Emotion classes\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "\n",
    "# Create directories for the new dataset\n",
    "if not os.path.exists(processed_folder):\n",
    "    os.makedirs(processed_folder)\n",
    "for emotion in emotions:\n",
    "    os.makedirs(os.path.join(processed_folder, emotion), exist_ok=True)\n",
    "\n",
    "target_width, target_height = 128, 128\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(base_dir, emotion)\n",
    "    all_images = os.listdir(emotion_dir)\n",
    "    existing_dir = os.path.join(processed_folder, emotion)\n",
    "    existing_images = os.listdir(existing_dir)\n",
    "    all_images = [img for img in all_images if img not in existing_images]\n",
    "\n",
    "    for image_name in all_images:\n",
    "        try:\n",
    "            img_path = os.path.join(emotion_dir, image_name)\n",
    "            # Load an image\n",
    "            img = cv2.imread(img_path)\n",
    "            image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect faces in the image\n",
    "            results = detector.detect_faces(image_rgb)\n",
    "            \n",
    "            if results:\n",
    "                # Extract the bounding box and facial landmarks\n",
    "                x, y, width, height = results[0]['box']\n",
    "                landmarks = results[0]['keypoints']\n",
    "                \n",
    "                # Calculate the angle to align eyes horizontally\n",
    "                left_eye = landmarks['left_eye']\n",
    "                right_eye = landmarks['right_eye']\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                angle = np.arctan(delta_y / delta_x) * 180 / np.pi\n",
    "                \n",
    "                # Center coordinates of the face\n",
    "                center = (x + width // 2, y + height // 2)\n",
    "                # Rotation matrix\n",
    "                rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "                # Perform the rotation on the entire image\n",
    "                aligned_image = cv2.warpAffine(img, rotation_matrix, (img.shape[1], img.shape[0]))\n",
    "                # Crop the face\n",
    "                aligned_face = aligned_image[y:y + height, x:x + width]\n",
    "                # Convert to grayscale\n",
    "                aligned_face_gray = cv2.cvtColor(aligned_face, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Convert the aligned grayscale face to PIL Image for resizing and padding\n",
    "                pil_image = Image.fromarray(aligned_face_gray)\n",
    "                \n",
    "                # Calculate the scaling factor to maintain aspect ratio\n",
    "                aspect_ratio = min(target_width / pil_image.width, target_height / pil_image.height)\n",
    "                new_width = int(pil_image.width * aspect_ratio)\n",
    "                new_height = int(pil_image.height * aspect_ratio)\n",
    "                # Resize the image with the scaling factor\n",
    "                pil_image = pil_image.resize((new_width, new_height), Image.LANCZOS)\n",
    "                \n",
    "                # Create a new image with padding\n",
    "                new_image = Image.new(\"L\", (target_width, target_height), (0))\n",
    "                paste_position = ((target_width - new_width) // 2, (target_height - new_height) // 2)\n",
    "                new_image.paste(pil_image, paste_position)\n",
    "                \n",
    "                save_path = os.path.join(processed_folder, emotion, image_name)\n",
    "                new_image.save(save_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2134070a80c87b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer Learning Model - Data Preparation\n",
    "Face Detection\n",
    "Image Cropping, Alignment and Grayscaling\n",
    "Rescaling and Padding\n",
    "224, 224, 3 grayscale images as expected by the pretrained models\n",
    "\n",
    "This code only needs to be run once to create the dataset used for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38c985aec7f6f16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize MTCNN detector\n",
    "detector = MTCNN()\n",
    "\n",
    "base_dir = \"Datasets/combined_dataset_faces_only_mtcnn\"\n",
    "processed_folder = \"Datasets/combined_dataset_processed_224_3\"\n",
    "\n",
    "# Emotion classes\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "\n",
    "# Create directories for the new dataset\n",
    "if not os.path.exists(processed_folder):\n",
    "    os.makedirs(processed_folder)\n",
    "for emotion in emotions:\n",
    "    os.makedirs(os.path.join(processed_folder, emotion), exist_ok=True)\n",
    "\n",
    "target_width, target_height = 224, 224\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(base_dir, emotion)\n",
    "    all_images = os.listdir(emotion_dir)\n",
    "    existing_dir = os.path.join(processed_folder, emotion)\n",
    "    existing_images = os.listdir(existing_dir)\n",
    "    all_images = [img for img in all_images if img not in existing_images]\n",
    "\n",
    "    for image_name in all_images:\n",
    "        try:\n",
    "            img_path = os.path.join(emotion_dir, image_name)\n",
    "            # Load an image\n",
    "            img = cv2.imread(img_path)\n",
    "            image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect faces in the image\n",
    "            results = detector.detect_faces(image_rgb)\n",
    "            \n",
    "            if results:\n",
    "                # Extract the bounding box and facial landmarks\n",
    "                x, y, width, height = results[0]['box']\n",
    "                landmarks = results[0]['keypoints']\n",
    "                \n",
    "                # Calculate the angle to align eyes horizontally\n",
    "                left_eye = landmarks['left_eye']\n",
    "                right_eye = landmarks['right_eye']\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                angle = np.arctan(delta_y / delta_x) * 180 / np.pi\n",
    "                \n",
    "                # Center coordinates of the face\n",
    "                center = (x + width // 2, y + height // 2)\n",
    "                # Rotation matrix\n",
    "                rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "                # Perform the rotation on the entire image\n",
    "                aligned_image = cv2.warpAffine(img, rotation_matrix, (img.shape[1], img.shape[0]))\n",
    "                # Crop the face\n",
    "                aligned_face = aligned_image[y:y + height, x:x + width]\n",
    "                # Convert to grayscale\n",
    "                aligned_face_gray = cv2.cvtColor(aligned_face, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Convert the aligned grayscale face to PIL Image for resizing and padding\n",
    "                pil_image = Image.fromarray(aligned_face_gray)\n",
    "                \n",
    "                # Calculate the scaling factor to maintain aspect ratio\n",
    "                aspect_ratio = min(target_width / pil_image.width, target_height / pil_image.height)\n",
    "                new_width = int(pil_image.width * aspect_ratio)\n",
    "                new_height = int(pil_image.height * aspect_ratio)\n",
    "                # Resize the image with the scaling factor\n",
    "                pil_image = pil_image.resize((new_width, new_height), Image.LANCZOS)\n",
    "                \n",
    "                # Create a new image with padding\n",
    "                new_image = Image.new(\"L\", (target_width, target_height), (0))\n",
    "                paste_position = ((target_width - new_width) // 2, (target_height - new_height) // 2)\n",
    "                new_image.paste(pil_image, paste_position)\n",
    "                \n",
    "                # Convert the single-channel grayscale image to 3-channel\n",
    "                new_image_rgb = new_image.convert(\"RGB\")\n",
    "                \n",
    "                save_path = os.path.join(processed_folder, emotion, image_name)\n",
    "                new_image_rgb.save(save_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7049ee74453492d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Landmark Model - Determine Landmarks\n",
    "Create files with 6 channels, 1 for grayscale image and 5 for facial features as NPY file. Eyebrows and eyes are combined into single channels since assysmetric features are not relevant in this model with this dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a358f4a62e6b5588"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = 'Datasets/combined_dataset_processed_128_1'\n",
    "new_data_dir = 'Datasets/combined_dataset_enhanced_npy_6_channels'\n",
    "shape_predictor_path = 'PretrainedModels/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "if not os.path.exists(new_data_dir):\n",
    "    os.makedirs(new_data_dir)\n",
    "\n",
    "# Initialize Dlib's detector and predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "def extract_landmarks_and_save(image_path, predictor, new_data_dir):\n",
    "    # Load the original image and prepare output directory and path\n",
    "    class_dir, image_name = os.path.split(image_path)\n",
    "    _, class_name = os.path.split(class_dir)\n",
    "    new_class_dir = os.path.join(new_data_dir, class_name)\n",
    "    if not os.path.exists(new_class_dir):\n",
    "        os.makedirs(new_class_dir)\n",
    "    # Adjust the file extension for saving as .npy\n",
    "    new_image_path = os.path.join(new_class_dir, image_name.replace('.jpg', '.npy'))\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Prepare an empty list to hold feature images\n",
    "    feature_images = [gray_image]  # Start with the grayscale image\n",
    "\n",
    "    # Assuming the image is a cropped face, create a rectangle covering the whole image\n",
    "    dlib_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for dlib\n",
    "    rect = dlib.rectangle(0, 0, gray_image.shape[1]-1, gray_image.shape[0]-1)\n",
    "    landmarks = predictor(dlib_img, rect)\n",
    "\n",
    "    # Define indices for facial features\n",
    "    feature_indices = {\n",
    "        'jawline': list(range(0, 17)),\n",
    "        # 'right_eyebrow': list(range(17, 22)),\n",
    "        # 'left_eyebrow': list(range(22, 27)),\n",
    "        'nose': list(range(27, 36)),\n",
    "        # 'right_eye': list(range(36, 42)),\n",
    "        # 'left_eye': list(range(42, 48)),\n",
    "        'mouth': list(range(48, 68))\n",
    "        # 'eyebrows': range(17, 27),  # Combining left and right eyebrows\n",
    "        # 'eyes': range(36, 48),  # Combining left and right eyes\n",
    "    }\n",
    "\n",
    "    # Create and draw feature images\n",
    "    for feature, indices in feature_indices.items():\n",
    "        # Initialize a single-channel image (black)\n",
    "        feature_img = np.zeros_like(gray_image)\n",
    "        points = np.array([[landmarks.part(n).x, landmarks.part(n).y] for n in indices], dtype=np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "        # Use polylines to draw the feature\n",
    "        cv2.polylines(feature_img, [points], isClosed=(feature != 'jawline'), color=255, thickness=1)\n",
    "        feature_images.append(feature_img)\n",
    "    \n",
    "    # Function to create feature image for separate features without connecting them\n",
    "    def create_feature_image(indices, gray_image):\n",
    "        feature_img = np.zeros_like(gray_image)\n",
    "        for n in indices:\n",
    "            x, y = landmarks.part(n).x, landmarks.part(n).y\n",
    "            cv2.circle(feature_img, (x, y), 1, 255, -1)  # Use cv2.circle to mark each point\n",
    "        return feature_img\n",
    "    \n",
    "    # Create temporary images for each eye and eyebrow\n",
    "    left_eyebrow_img = create_feature_image(list(range(22, 27)), gray_image)\n",
    "    right_eyebrow_img = create_feature_image(list(range(17, 22)), gray_image)\n",
    "    left_eye_img = create_feature_image(list(range(42, 48)), gray_image)\n",
    "    right_eye_img = create_feature_image(list(range(36, 42)), gray_image)\n",
    "    \n",
    "    # Combine temporary images for eyebrows and eyes into single images\n",
    "    combined_eyebrows_img = cv2.bitwise_or(left_eyebrow_img, right_eyebrow_img)\n",
    "    combined_eyes_img = cv2.bitwise_or(left_eye_img, right_eye_img)\n",
    "    \n",
    "    # Append the combined images to the feature_images list\n",
    "    feature_images.append(combined_eyebrows_img)\n",
    "    feature_images.append(combined_eyes_img)\n",
    "\n",
    "    # Stack the single-channel images to form a multi-channel image\n",
    "    multi_channel_image = np.stack(feature_images, axis=-1)\n",
    "\n",
    "    # Save the multi-channel image as NPY\n",
    "    np.save(new_image_path, multi_channel_image)\n",
    "\n",
    "def process_directory_with_landmarks(directory, new_data_dir, predictor):\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(directory)])\n",
    "    processed_files = 0\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                extract_landmarks_and_save(image_path, predictor, new_data_dir)\n",
    "                processed_files += 1\n",
    "                progress_percentage = (processed_files / total_files) * 100\n",
    "                print(f\"\\rProcessed {processed_files}/{total_files} files ({progress_percentage:.2f}%)\", end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    print(\"\\nFinished processing all files.\")\n",
    "\n",
    "# Process the directory and save images with additional channels\n",
    "process_directory_with_landmarks(data_dir, new_data_dir, predictor)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12565918320bd21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualisation of the generated files for the landmark model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103eecb13aafae6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def display_image_and_channels(image_data):\n",
    "    n_channels = image_data.shape[2]\n",
    "    fig, axs = plt.subplots(1, n_channels, figsize=(15, 5))\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        # For grayscale visualization, ensure the single channel is repeated 3 times\n",
    "        if i == 0:  # Assuming the first channel is the grayscale original image\n",
    "            axs[i].imshow(image_data[:, :, i], cmap='gray')\n",
    "        else:\n",
    "            # Visualize the additional channels, which represent the facial features\n",
    "            # Here, a mask is used to only display the areas of interest\n",
    "            mask = image_data[:, :, i] > 0  # Creating a mask where the feature is drawn\n",
    "            display_img = np.zeros(image_data[:, :, 0].shape + (3,), dtype=np.uint8)  # Prepare a blank RGB image\n",
    "            display_img[..., 1][mask] = 255  # Draw the feature in green on the mask\n",
    "            axs[i].imshow(display_img)\n",
    "        axs[i].axis('off')  # Hide axes for better visualization\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def load_and_display_random_images(directory, num_images=15):\n",
    "    # List all .npy files in the specified directory\n",
    "    all_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "    \n",
    "    # Select a random subset of files\n",
    "    selected_files = random.sample(all_files, min(len(all_files), num_images))\n",
    "    \n",
    "    # Load and display each selected file\n",
    "    for file_path in selected_files:\n",
    "        print(f\"Displaying: {file_path}\")\n",
    "        image_data = np.load(file_path)\n",
    "        display_image_and_channels(image_data)\n",
    "\n",
    "# Specify the directory containing the .npy files for the \"happy\" class\n",
    "npy_directory = 'Datasets/combined_dataset_enhanced_npy/happy/'\n",
    "\n",
    "# Load and display 15 random images and their channels\n",
    "load_and_display_random_images(npy_directory, num_images=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0fa85bfd96f8ddc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
