{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf16881649162cab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf3b441f6c1308e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define paths to your training and testing directories\n",
    "# data_dir = 'Datasets/combined_dataset_processed'\n",
    "data_dir = 'Datasets/combined_dataset_enhanced_npy_6_channels'\n",
    "\n",
    "# Function to add images from a directory to a list\n",
    "def process_directory(directory, data_list):\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Loop through each image in the folder\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                if image_name.endswith('.npy'):  # Ensure the file is a .npy file\n",
    "                    image_path = os.path.join(class_dir, image_name)\n",
    "                    image = np.load(image_path)\n",
    "\n",
    "                    # Check if the image is of size 128x128 with 6 channels\n",
    "                    if image.shape == (128, 128, 6):\n",
    "                        # Append to the data list\n",
    "                        data_list.append({'filepath': image_path, 'label': class_name})\n",
    "\n",
    "# Initialize an empty list for storing data\n",
    "data_list = []\n",
    "# Add training images to the data list\n",
    "process_directory(data_dir, data_list)\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data_list)\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Print the amount of images per category before balancing\n",
    "print(\"Images per category before balancing:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_df, temp_test_val_df = train_test_split(df, train_size=0.7, random_state=42)\n",
    "test_df, val_df = train_test_split(temp_test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Balance the test and validation sets\n",
    "# Determine the smallest class size within each of the test and validation sets\n",
    "min_test_class_size = test_df['label'].value_counts().min()\n",
    "min_val_class_size = val_df['label'].value_counts().min()\n",
    "\n",
    "# Determine the smallest size between the two for a uniform approach\n",
    "uniform_min_size = min(min_test_class_size, min_val_class_size)\n",
    "\n",
    "# Function to reduce class sizes\n",
    "def balance_classes(df, target_size):\n",
    "    balanced_df = pd.DataFrame()  # Initialize an empty DataFrame to hold the balanced data\n",
    "    for label in df['label'].unique():\n",
    "        subset = df[df['label'] == label].sample(n=target_size, random_state=42)\n",
    "        balanced_df = pd.concat([balanced_df, subset])\n",
    "    return balanced_df\n",
    "\n",
    "# Apply balancing\n",
    "test_df = balance_classes(test_df, uniform_min_size)\n",
    "val_df = balance_classes(val_df, uniform_min_size)\n",
    "\n",
    "# Calculate and print split ratios\n",
    "total_samples = len(df)\n",
    "train_ratio = len(train_df) / total_samples\n",
    "val_ratio = len(val_df) / total_samples\n",
    "test_ratio = len(test_df) / total_samples\n",
    "\n",
    "print(f\"\\nTotal samples: {total_samples}\")\n",
    "print(f\"Training set: {train_ratio:.2f} ({len(train_df)} samples)\")\n",
    "print(f\"Validation set: {val_ratio:.2f} ({len(val_df)} samples)\")\n",
    "print(f\"Test set: {test_ratio:.2f} ({len(test_df)} samples)\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bc7a1a375d830e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237a4aa183ad0e4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set the image size and batch size\n",
    "image_size = (128, 128)\n",
    "batch_size = 64 \n",
    "\n",
    "# Encode the labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['encoded_label'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['encoded_label'] = label_encoder.transform(val_df['label'])\n",
    "test_df['encoded_label'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "def set_shapes(img, label, img_shape=(128, 128, 6), label_shape=(7, )):\n",
    "    img.set_shape(img_shape)\n",
    "    label.set_shape(label_shape)\n",
    "    return img, label\n",
    "\n",
    "def preprocess_npy(file_path, label):\n",
    "    # Load .npy file\n",
    "    image = np.load(file_path.numpy())\n",
    "    # Normalize image data\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    # One-hot encode the label\n",
    "    label = tf.one_hot(label, depth=7)  # 7 classes\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(df, batch_size):\n",
    "    file_paths = df['filepath'].values\n",
    "    labels = df['encoded_label'].values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(preprocess_npy, [x, y], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(set_shapes)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_df, batch_size)\n",
    "val_dataset = create_dataset(val_df, batch_size)\n",
    "test_dataset = create_dataset(test_df, batch_size)\n",
    "\n",
    "# Calculate class weights for the training data\n",
    "labels_for_class_weight = train_df['encoded_label'].values\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(labels_for_class_weight),\n",
    "                                     y=labels_for_class_weight)\n",
    "\n",
    "# Convert class weights to a dictionary to pass to model.fit\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(class_weight_dict)\n",
    "\n",
    "# for images, labels in train_dataset.take(1):  # Take a single batch from the dataset\n",
    "#     print(\"Image batch shape:\", images.shape)\n",
    "#     print(\"Label batch shape:\", labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc311c5c354e270"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdcfb3006eb0ada9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 6)\n",
    "l2_reg = 0.001  # Regularization strength\n",
    "dropout = [0.25, 0.4]\n",
    "n_filters = [128, 64, 32, 128]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    # First Conv Block\n",
    "    Conv2D(n_filters[0], (3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg)), \n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(dropout[0]),\n",
    "    # Second Conv Block\n",
    "    Conv2D(n_filters[1], (3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(dropout[0]),\n",
    "    # Third Conv Block\n",
    "    Conv2D(n_filters[2], (3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(dropout[0]),\n",
    "    # Fourth Conv Block\n",
    "    Conv2D(n_filters[1], (3, 3), padding='same', activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(dropout[0]),\n",
    "    # Flatten and Dense Layers\n",
    "    Flatten(),\n",
    "    Dense(n_filters[3], activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dropout[1]),\n",
    "    Dense(7, activation='softmax', kernel_regularizer=l2(l2_reg))  # 7 classes for the output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7fabe96cce7e1e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Callbacks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba219b1ca965590"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting. This stops training when the model's performance on the validation set starts to degrade.\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to be monitored\n",
    "    patience=3,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "# ModelCheckpoint callback\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f'logs/model_checkpoint_{timestamp}.keras',  # Path where to save the model\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    save_best_only=False,    # Save only the best model. Set False to save the model at the end of every epoch so restarting from specific epoch is possible\n",
    "    save_weights_only=False, # Save only the weights\n",
    "    mode='min',             # Minimize the monitored metric (val_loss) min before\n",
    "    verbose=1               # Verbose output\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=2,\n",
    "    min_lr=0.0001,\n",
    "    cooldown=3,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac4a09040263e1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca68801975535ab7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the last saved weights\n",
    "# model.load_weights('logs/model_checkpoint_20240225_085054.keras')\n",
    "\n",
    "epochs = 50  # When resuming training, set epochs to the total number of epochs you want to train, not just the additional epochs. The model.fit() method continues training for the specified number of epochs, starting from the current epoch count.\n",
    "\n",
    "steps_per_epoch = np.ceil(len(train_df) / batch_size).astype(int)\n",
    "validation_steps = np.ceil(len(val_df) / batch_size).astype(int)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,  # Training data\n",
    "    epochs=epochs,  # Number of epochs\n",
    "    validation_data=val_dataset,  # Validation data\n",
    "    class_weight=class_weight_dict,  # Class weights\n",
    "    steps_per_epoch=len(train_df) // batch_size,  # Number of steps per epoch\n",
    "    validation_steps=len(val_df) // batch_size,  # Number of validation steps\n",
    "    verbose=1,  # Show training log\n",
    "    callbacks=[early_stopper, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the training history for later analysis\n",
    "with open(f'logs/training_history_{timestamp}.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e63c731c5cc4f51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation and Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f772fe6df94d60d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_steps = np.ceil(len(test_df) / batch_size).astype(int)\n",
    "\n",
    "# Ensure test_dataset is correctly prepared with batching and without shuffling for evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=test_steps)\n",
    "print(\"Test accuracy: \", test_accuracy)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = model.predict(test_dataset, steps=test_steps)\n",
    "predicted_classes = np.argmax(predictions, axis=1)  # because of softmax output from model\n",
    "\n",
    "# If true labels are one-hot encoded, convert them to integer labels\n",
    "true_labels_integer = np.concatenate([y.numpy() for x, y in test_dataset.unbatch().batch(batch_size).take(test_steps)], axis=0)\n",
    "if true_labels_integer.ndim > 1:  # Checking if labels are one-hot encoded\n",
    "    true_labels_integer = np.argmax(true_labels_integer, axis=1)\n",
    "\n",
    "# Ensure true_labels and predicted_classes arrays are aligned in length\n",
    "true_labels_integer = true_labels_integer[:len(predicted_classes)]\n",
    "\n",
    "# Assuming 'label_encoder' is defined and used for encoding labels\n",
    "class_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels_integer, predicted_classes, target_names=class_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels_integer, predicted_classes)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "plt.yticks(tick_marks, class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a81fe9421475b249"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "86374dd9531cfe1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conducting error analysis\n",
    "This can be done by examining misclassified examples, which can provide insights into what types of errors the model is making"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8753732a99f4f02c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(f'logs/training_history_20240301_170452.pkl', 'rb') as file:\n",
    "    history = pickle.load(file)\n",
    "# Learning Curves\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f810fd7fdecc1ce1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Precsion-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the labels for multi-class\n",
    "y_bin = label_binarize(true_labels_integer, classes=np.arange(len(class_labels)))\n",
    "n_classes = y_bin.shape[1]\n",
    "\n",
    "# Compute precision-recall curve for each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], predictions[:, i])\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], precision[i], lw=2, label='Class {}'.format(class_labels[i]))\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Precision vs. Recall curve\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3ae057595eb5619"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ROC Curve and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='Class {} (area = {:.2f})'.format(class_labels[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78a567de751d0acf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
